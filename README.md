# NLP_Proejct_2023
 1 Implementing Word2Vec
In this part you will implement the word2vec model and train your own word vectors with stochastic
gradient descent (SGD). Numpy methods could be u􀆟lized to make your code both shorter and faster.
The following requirements should be sa􀆟sfied:
a) Nega􀆟ve sampling loss
b) Implement the skip-gram model from scratch
c) Train with real-data
d) Show the resulting embeddings
2 Neural Machine Translation
a) Implement the below model
b) Use a small set of real machine transla􀆟on data
c) Test with some (very similar) sentences
![image](https://github.com/Hakulani/NLP_Proejct_2023/assets/61573397/7bd01a3f-534d-4b0b-b0bb-cbf057ea5b8d)
3. Implement A Simple Transformer Model
a) Implement a small transformer with one layer encoder and one layer decoder with self-attention
according to the “A􀆩en􀆟on is All you Need” paper
b) A sub-layer can be constructed from code available in any package
c) Show and explain results (input, output) of each sub-layer during training and testing
4. Implement A Named En􀆟ty Recognizer (Bi-LSTM + CRF)
a) Implement according to h􀆩ps://aclanthology.org/N16-1030/, using a dataset from
https://nlpforthai.com/tasks/ner/ (a very small subset is sufficient for demonstra􀆟on)
b) You can put together the model from code available in any package

5 Music Genera􀆟on by GPT
a) You can use GPT code from any package
b) Training uses MIDI files, downloadable from the web (only main, catchy parts of a song may be
used)
c) Generate and play the generated tunes
